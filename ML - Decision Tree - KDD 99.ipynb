{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Decision Tree Classifier\n",
    "\n",
    "In this notebook, we explore the use of decision tree classifier for a Network Intrusion detection use case. The data set used is from KDD cup 1999. \n",
    "\n",
    "http://www.kdd.org/kdd-cup/view/kdd-cup-1999\n",
    "\n",
    "The dataset is a version from the 1998 DARPA Intrusion Detection Evaluation Program by MIT Lincoln Labs. The draw TCP dump ata was collected from a LAN in a lab environment over nine weeks. Simulated attacks were performed to generate malicious traffic.\n",
    "\n",
    "Attacks could be classified into four main categories:\n",
    "\n",
    "DOS: denial-of-service\n",
    "R2L: Remote to Local - unauthorized access from a remote machine \n",
    "U2R: User to Root\n",
    "probing: surveillance and other probing like port scanning.\n",
    "\n",
    "Step 1. Load the Data\n",
    "\n",
    "The schema structure is defined and the data is loaded into a spark DataFrame from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType( [\n",
    "    \n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"protocol_type\", StringType(), True),\n",
    "    StructField(\"service\", StringType(), True),\n",
    "    StructField(\"flag\", StringType(), True),\n",
    "    StructField(\"src_bytes\", DoubleType(), True),\n",
    "    StructField(\"dst_bytes\", DoubleType(), True),\n",
    "    StructField(\"land\", DoubleType(), True),\n",
    "    StructField(\"wrong_fragment\", DoubleType(), True),\n",
    "    StructField(\"urgent\", DoubleType(), True),\n",
    "    StructField(\"hot\", DoubleType(), True),\n",
    "    StructField(\"num_failed_logins\", DoubleType(), True),\n",
    "    StructField(\"logged_in\", DoubleType(), True),\n",
    "    StructField(\"num_compromised\", DoubleType(), True),\n",
    "    StructField(\"root_shell\", DoubleType(), True),\n",
    "    StructField(\"su_attempted\", DoubleType(), True),\n",
    "    StructField(\"num_root\", DoubleType(), True),\n",
    "    StructField(\"num_file_creations\", DoubleType(), True),\n",
    "    StructField(\"num_shells\", DoubleType(), True),\n",
    "    StructField(\"num_access_files\", DoubleType(), True),\n",
    "    StructField(\"num_outbound_cmds\", DoubleType(), True),\n",
    "    StructField(\"is_host_login\", DoubleType(), True),\n",
    "    StructField(\"is_guest_login\", DoubleType(), True),\n",
    "    StructField(\"count\", DoubleType(), True),\n",
    "    StructField(\"srv_count\", DoubleType(), True),\n",
    "    StructField(\"serror_rate\", DoubleType(), True),\n",
    "    StructField(\"srv_serror_rate\", DoubleType(), True),\n",
    "    StructField(\"rerror_rate\", DoubleType(), True),\n",
    "    StructField(\"srv_rerror_rate\", DoubleType(), True),\n",
    "    StructField(\"same_srv_rate\", DoubleType(), True),\n",
    "    StructField(\"diff_srv_rate\", DoubleType(), True),\n",
    "    StructField(\"srv_diff_host_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_count\", DoubleType(), True),\n",
    "    StructField(\"dst_host_srv_count\", DoubleType(), True),\n",
    "    StructField(\"dst_host_same_srv_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_diff_srv_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_same_src_port_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_srv_diff_host_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_serror_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_srv_serror_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_rerror_rate\", DoubleType(), True),\n",
    "    StructField(\"dst_host_srv_rerror_rate\", DoubleType(), True),\n",
    "    StructField(\"label\", StringType(), True)\n",
    "])\n",
    "\n",
    "kdd10_df = spark.read.csv(\"/home/training/bdpgstudent/data/kddcup.data_10_percent\", schema=schema)\n",
    "\n",
    "kdd10_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are close to 500,000 records in this data set, which is a smaller version of the original data set.\n",
    "\n",
    "Step 2\n",
    "\n",
    "Split the data set into two, one for training the model, and the other one for verification\n",
    "\n",
    "Persist the data, to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_data, test_data = kdd10_df.randomSplit([0.7, 0.3], seed=1)\n",
    "\n",
    "training_data.persist()\n",
    "\n",
    "training_data.count()\n",
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Create Tranformers, Evaluator and Pipeline\n",
    "\n",
    "We will now create a features vector. The features vector only takes numerical value as input. But the label column has String values. So we use StringIndexer, to create a new column with mapped numeric values.\n",
    "\n",
    "Next we create the classifer and also a pipeline with three stages to execute the transformation and evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Transformers\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "input_cols = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "               'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "               'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', \n",
    "               'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "               'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "               'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', \n",
    "               'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', \n",
    "               'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "featuresCreator = ft.VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "label_indexer = ft.StringIndexer(inputCol=\"label\", outputCol=\"label_num\")\n",
    "\n",
    "import pyspark.ml.classification as cl\n",
    "dt = cl.DecisionTreeClassifier(labelCol=\"label_num\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline( stages = [ featuresCreator,label_indexer, dt] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Fit the model\n",
    "\n",
    "In this step, we fit our model with training data. The fit function of pipeline will take the input data, take the data through the stages and the output is a trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Evaluate the model\n",
    "\n",
    "In this step, the trained model is evaluated with test data set. Then the predictions by the model based on test data is evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9890905163102182\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_num\", \n",
    "        metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the percentage accuracy\n",
    "\n",
    "Now we will print the decision tree rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4f9582349f5257734116) of depth 5 with 31 nodes\n",
      "  If (feature 20 <= 444.5)\n",
      "   If (feature 25 <= 0.32)\n",
      "    If (feature 31 <= 0.14500000000000002)\n",
      "     If (feature 1 <= 0.5)\n",
      "      If (feature 32 <= 0.065)\n",
      "       Predict: 1.0\n",
      "      Else (feature 32 > 0.065)\n",
      "       Predict: 7.0\n",
      "     Else (feature 1 > 0.5)\n",
      "      If (feature 1 <= 6.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 1 > 6.5)\n",
      "       Predict: 2.0\n",
      "    Else (feature 31 > 0.14500000000000002)\n",
      "     If (feature 1 <= 6.5)\n",
      "      If (feature 23 <= 0.995)\n",
      "       Predict: 4.0\n",
      "      Else (feature 23 > 0.995)\n",
      "       Predict: 7.0\n",
      "     Else (feature 1 > 6.5)\n",
      "      If (feature 19 <= 75.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 19 > 75.5)\n",
      "       Predict: 8.0\n",
      "   Else (feature 25 > 0.32)\n",
      "    If (feature 19 <= 275.5)\n",
      "     If (feature 9 <= 0.5)\n",
      "      If (feature 4 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 4 > 0.5)\n",
      "       Predict: 8.0\n",
      "     Else (feature 9 > 0.5)\n",
      "      If (feature 30 <= 0.935)\n",
      "       Predict: 2.0\n",
      "      Else (feature 30 > 0.935)\n",
      "       Predict: 3.0\n",
      "    Else (feature 19 > 275.5)\n",
      "     If (feature 1 <= 518.0)\n",
      "      Predict: 2.0\n",
      "     Else (feature 1 > 518.0)\n",
      "      Predict: 0.0\n",
      "  Else (feature 20 > 444.5)\n",
      "   If (feature 1 <= 518.0)\n",
      "    Predict: 2.0\n",
      "   Else (feature 1 > 518.0)\n",
      "    Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = model.stages[2]\n",
    "print(dtree.toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
