{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Logistics Regression\n",
    "### (Using Logistics Regression to see if the model performance is better than Optimized Decision Tree (Abhi-Churn-DT-Optimized-1.0.pynb)\n",
    "\n",
    "Retaining existing customers is an important aspect of running a successful business. The cost of acquiring a new customer is significantly high, and it is imperative for any business to retain its existing customers. Loyal customers are more profitable and they bring in steady streams of revenue\n",
    "\n",
    "## Research Goal\n",
    "\n",
    "If there is a way to identify the customers with higher probability of attrition, various strategies to retain them could be implemented. In this analysis, we will use machine learning algorithm (Logistics Regressiob) to build a model, which could predict customer churn.\n",
    "\n",
    "In this study we use Orange Telecom’s dataset. It is a cleansed dataset, with customer activity information as features and a label specifying if the customer cancelled subscription (churn)\n",
    "\n",
    "https://github.com/caroljmcdonald/mapr-sparkml-churn/tree/master/data\n",
    "\n",
    "There are two data sets \n",
    "\n",
    "churn80 (training) \n",
    "\n",
    "churn20(test)\n",
    "\n",
    "Reference – https://dzone.com/articles/churn-prediction-with-apache-spark-machine-learnin\n",
    "\n",
    "### SCHEMA\n",
    "\n",
    "State: string\n",
    "\n",
    "Account length: integer\n",
    "\n",
    "Area code: integer\n",
    "\n",
    "International plan: string\n",
    "\n",
    "Voice mail plan: string\n",
    "\n",
    "Number vmail messages: integer\n",
    "\n",
    "Total day minutes: double\n",
    "\n",
    "Total day calls: integer\n",
    "\n",
    "Total day charge: double\n",
    "\n",
    "Total eve minutes: double\n",
    "\n",
    "Total eve calls: integer\n",
    "\n",
    "Total eve charge: double\n",
    "\n",
    "Total night minutes: double\n",
    "\n",
    "Total night calls: integer\n",
    "\n",
    "Total night charge: double\n",
    "\n",
    "Total international minutes: double\n",
    "\n",
    "Total international calls: integer\n",
    "\n",
    "Total international charge: double\n",
    "\n",
    "Customer service calls: integer\n",
    "\n",
    "Churn: String\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below step, the data from file location is loaded into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"len\", IntegerType(), True),\n",
    "    StructField(\"acode\", StringType(), True),\n",
    "    StructField(\"intlplan\", StringType(), True),\n",
    "    StructField(\"vplan\", StringType(), True),\n",
    "    StructField(\"numvmail\", DoubleType(), True),\n",
    "    StructField(\"tdmins\", DoubleType(), True),\n",
    "    StructField(\"tdcalls\", DoubleType(), True),\n",
    "    StructField(\"tdcharge\", DoubleType(), True),\n",
    "    StructField(\"temins\", DoubleType(), True),\n",
    "    StructField(\"tecalls\", DoubleType(), True),\n",
    "    StructField(\"techarge\", DoubleType(), True),\n",
    "    StructField(\"tnmins\", DoubleType(), True),\n",
    "    StructField(\"tncalls\", DoubleType(), True),\n",
    "    StructField(\"tncharge\", DoubleType(), True),\n",
    "    StructField(\"timins\", DoubleType(), True),\n",
    "    StructField(\"ticalls\", DoubleType(), True),\n",
    "    StructField(\"ticharge\", DoubleType(), True),\n",
    "    StructField(\"numcs\", DoubleType(), True),\n",
    "    StructField(\"churn\", StringType(), True)\n",
    "                    ])\n",
    "\n",
    "churn20_df = spark.read.csv(\"/home/training/bdpgstudent/data/churn/churn-bigml-20.csv\", schema=schema, header = True)\n",
    "\n",
    "churn80_df = spark.read.csv(\"/home/training/bdpgstudent/data/churn/churn-bigml-80.csv\",  schema=schema, header = True)\n",
    "churn80_df.count()\n",
    "\n",
    "#churn80_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn20_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "In below steps, we perform Data Exploration. Aggregate of call minutes and charges are computed and printed.\n",
    "\n",
    "Also, to find the correlation between fields, a Correlation Matrix is printed in tabular form. The intention is to identify the fields with close correlation and remove them from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------+-----------------+\n",
      "|        tdmintotal|    tdchargetotal|       temintotal|    techargetotal|\n",
      "+------------------+-----------------+-----------------+-----------------+\n",
      "|478498.00000000023|81346.07000000011|534229.5000000003|45410.17000000004|\n",
      "+------------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "╒══════════╤══════════╤════════════╤════════════╤════════════╤════════════╤════════════╤═════════════╤═════════════╕\n",
      "│          │  tdmins  │  tdcharge  │   temins   │  techarge  │   tnmins   │  tncharge  │   timins    │  ticharge   │\n",
      "╞══════════╪══════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═════════════╪═════════════╡\n",
      "│ tdmins   │    1     │     1      │ 0.00399863 │ 0.00399248 │  0.013491  │ 0.0134638  │  -0.011042  │  -0.010934  │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ tdcharge │          │     1      │ 0.00400833 │ 0.00400219 │ 0.0134949  │ 0.0134678  │ -0.0110461  │ -0.0109382  │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ temins   │          │            │     1      │     1      │ -0.0134141 │ -0.0134496 │ -0.00691466 │ -0.00694749 │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ techarge │          │            │            │     1      │ -0.013428  │ -0.0134636 │ -0.00692257 │ -0.00695543 │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ tnmins   │          │            │            │            │     1      │  0.999999  │ -0.00860744 │ -0.00850976 │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ tncharge │          │            │            │            │            │     1      │ -0.00861468 │ -0.00851701 │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ timins   │          │            │            │            │            │            │      1      │  0.999993   │\n",
      "├──────────┼──────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────────┼─────────────┤\n",
      "│ ticharge │          │            │            │            │            │            │             │      1      │\n",
      "╘══════════╧══════════╧════════════╧════════════╧════════════╧════════════╧════════════╧═════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "#kdd10_df.show()\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as fn\n",
    "test_df = churn80_df.agg( fn.sum('tdmins').alias('tdmintotal'), fn.sum('tdcharge').alias('tdchargetotal'),\n",
    "                         fn.sum('temins').alias('temintotal'),fn.sum('techarge').alias('techargetotal')\n",
    "                        )\n",
    "test_df.show()\n",
    "\n",
    "numerical = ['tdmins', 'tdcharge', 'temins', 'techarge', 'tnmins', 'tncharge', 'timins', 'ticharge']\n",
    "\n",
    "n_numerical = len(numerical)\n",
    "corr = []\n",
    "\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(churn80_df.corr(numerical[i], numerical[j]))\n",
    "    corr.append(temp)\n",
    "\n",
    "# print the correlation matrix in a nicely formatted table\n",
    "from tabulate import tabulate\n",
    "print(tabulate(corr, headers=numerical, showindex=numerical, tablefmt=\"fancy_grid\", numalign=\"center\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above step, it is evident that the fields Call Minutes and Call Charges have direct correlation. So one of the field can be removed from the model. In below step, we remove the call charge features and also a few other features (state, area code...). \n",
    "\n",
    "In the non optimized Decision Tree model, we had removed customer servicing calls, number of voice mails and length of account. We will not remove customer servicing calls, number of voice mail and length of account to compare the prediction of Logistics Regression Model with Decision Tree Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- len: integer (nullable = true)\n",
      " |-- intlplan: string (nullable = true)\n",
      " |-- numvmail: double (nullable = true)\n",
      " |-- tdmins: double (nullable = true)\n",
      " |-- tdcalls: double (nullable = true)\n",
      " |-- temins: double (nullable = true)\n",
      " |-- tecalls: double (nullable = true)\n",
      " |-- tnmins: double (nullable = true)\n",
      " |-- tncalls: double (nullable = true)\n",
      " |-- timins: double (nullable = true)\n",
      " |-- ticalls: double (nullable = true)\n",
      " |-- numcs: double (nullable = true)\n",
      " |-- churn: string (nullable = true)\n",
      "\n",
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|False| 2278|\n",
      "| True|  388|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove correlated columns and area code state code\n",
    "\n",
    "churn80_df_d = churn80_df.drop(\"state\").drop(\"acode\").drop(\"vplan\")\\\n",
    "                            .drop(\"tdcharge\").drop(\"techarge\")\\\n",
    "                            .drop(\"tncharge\").drop(\"ticharge\")\n",
    "churn20_df_d = churn20_df.drop(\"state\").drop(\"acode\").drop(\"vplan\")\\\n",
    "                        .drop(\"tdcharge\").drop(\"techarge\")\\\n",
    "                        .drop(\"tncharge\").drop(\"ticharge\")\n",
    "\n",
    "churn80_df_d.printSchema()\n",
    "churn80_df_d.groupBy(\"churn\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "From the count above we see that there are roughly about six times False churn samples than True Churn samples. We have to make the the model sensitive to Churn = True, so that model predicts the customers likely to leave. We use stratified sampling as shown below to keep all the Churn=True samples and randomly select Churn=False samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|False|  377|\n",
      "| True|  388|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "churn80_df_d_s = churn80_df_d.sampleBy(\"churn\", fractions={\"False\": 0.17, \"True\": 1}, seed=0)\n",
    "churn80_df_d_s.groupBy(\"churn\").count().orderBy(\"churn\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Transformers\n",
    "In below step, we create transformers to transform Data.\n",
    "\n",
    "The model will only accept numeric values. So we use a StrinIndexer to convert some String fields to integer values. We are interested in Churn feature, as it is the field that indicates if the customer is likely to leave or not. We are also interested in internatonal plan field, which is a String field.\n",
    "\n",
    "Then we create a features vector, with input columns required for the model computation, using VectorAssember.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "\n",
    "label_indexer = ft.StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "ip_indexer = ft.StringIndexer(inputCol=\"intlplan\", outputCol=\"iplanIndex\")\n",
    "\n",
    "\n",
    "input_cols = [\"len\", \"numvmail\",\"numcs\",\"iplanIndex\", \"tdmins\",\n",
    "     \"tdcalls\", \"temins\", \"tecalls\", \"tnmins\", \"tncalls\", \"timins\",\n",
    "     \"ticalls\"]\n",
    "\n",
    "featuresCreator = ft.VectorAssembler(inputCols=input_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Estimator\n",
    "\n",
    "We create a classifier model with a label column. This label column will be used by the classifier to classify the dataset.\n",
    "Here we are using a Decision Tree Classifer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating an estimator\n",
    "import pyspark.ml.classification as cl\n",
    "lr = cl.LogisticRegression(regParam=0.01, maxIter=50, elasticNetParam=0.01, labelCol=\"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline\n",
    "Now, create a Pipeline to pull the different transformations together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline( stages = [ label_indexer, ip_indexer, featuresCreator, lr] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model\n",
    "Now we run the pipeline to estimate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|label|\n",
      "+-----+-----+\n",
      "|False|  1.0|\n",
      "| True|  0.0|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model = pipeline.fit(churn80_df_d_s)\n",
    "\n",
    "#test the model with 20% test data\n",
    "predictions = model.transform(churn20_df_d)\n",
    "\n",
    "#printing the indexed label value of Churn Feature\n",
    "label_mapping = predictions.select(\"churn\",\"label\").distinct()\n",
    "label_mapping.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "Now we use a MultiClassClassificationEvaluator to evaluate the accuracy of the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7466266866566716\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", \n",
    "        metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric show 74.6% for Logistics Regression Model. For Decision Tree, we got about 86.05% Accuracy for the same dataset and same featuers (Abhi-Churn-DT-Optimized-1.0.pynb). So it is evident that Decision Tree model performs better for this use case, with the same data set and feature set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
